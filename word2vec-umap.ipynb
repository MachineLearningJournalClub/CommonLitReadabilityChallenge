{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "commonlitreadabilityprize/test.csv\n",
      "commonlitreadabilityprize/train.csv\n",
      "commonlitreadabilityprize/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('commonlitreadabilityprize/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/simoneazeglio/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "## more common imports\n",
    "from collections import Counter\n",
    "import re\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "\n",
    "# languange processing imports\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from gensim.corpora import Dictionary\n",
    "# preprocessing imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# model imports\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# hyperparameter training imports\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# visualization imports\n",
    "import umap\n",
    "#import umap.umap_ as umap\n",
    "from IPython.display import display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import base64\n",
    "import io\n",
    "%matplotlib inline\n",
    "sns.set()  # defines the style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find and remove non-ascii words\n",
    "# Non so se serva, ma si può modificare aggiungendo preprocessing ad-hoc\n",
    "\n",
    "our_special_word = 'qwerty'\n",
    "\n",
    "def remove_ascii_words(df):\n",
    "    \"\"\" removes non-ascii characters from the 'texts' column in df.\n",
    "    It returns the words containig non-ascii characers.\n",
    "    \"\"\"\n",
    "    non_ascii_words = []\n",
    "    for i in range(len(df)):\n",
    "        for word in df.loc[i, 'excerpt'].split(' '):\n",
    "            if any([ord(character) >= 128 for character in word]):\n",
    "                non_ascii_words.append(word)\n",
    "                df.loc[i, 'excerpt'] = df.loc[i, 'excerpt'].replace(word, our_special_word)\n",
    "    return non_ascii_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non so se serva, ma si può modificare aggiungendo preprocessing ad-hoc\n",
    "def get_good_tokens(sentence):\n",
    "    replaced_punctation = list(map(lambda token: re.sub('[^0-9A-Za-z!?]+', '', token), sentence))\n",
    "    removed_punctation = list(filter(lambda token: token, replaced_punctation))\n",
    "    return removed_punctation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we get transform the documents into sentences for the word2vecmodel\n",
    "# we made a function such that later on when we make the submission, we don't need to write duplicate code\n",
    "def w2v_preprocessing(df):\n",
    "    \"\"\" All the preprocessing steps for word2vec are done in this function.\n",
    "    All mutations are done on the dataframe itself. So this function returns\n",
    "    nothing.\n",
    "    \"\"\"\n",
    "    df['excerpt'] = df.excerpt.str.lower()\n",
    "    df['document_sentences'] = df.excerpt.str.split('.')  # split texts into individual sentences\n",
    "    df['tokenized_sentences'] = list(map(lambda sentences:\n",
    "                                         list(map(nltk.word_tokenize, sentences)),\n",
    "                                         df.document_sentences))  # tokenize sentences\n",
    "    df['tokenized_sentences'] = list(map(lambda sentences:\n",
    "                                         list(map(get_good_tokens, sentences)),\n",
    "                                         df.tokenized_sentences))  # remove unwanted characters\n",
    "    df['tokenized_sentences'] = list(map(lambda sentences:\n",
    "                                         list(filter(lambda lst: lst, sentences)),\n",
    "                                         df.tokenized_sentences))  # remove empty lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url_legal</th>\n",
       "      <th>license</th>\n",
       "      <th>excerpt</th>\n",
       "      <th>target</th>\n",
       "      <th>standard_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c12129c31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When the young people returned to the ballroom...</td>\n",
       "      <td>-0.340259</td>\n",
       "      <td>0.464009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85aa80a4c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All through dinner time, Mrs. Fayre was somewh...</td>\n",
       "      <td>-0.315372</td>\n",
       "      <td>0.480805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b69ac6792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As Roger had predicted, the snow departed as q...</td>\n",
       "      <td>-0.580118</td>\n",
       "      <td>0.476676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dd1000b26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And outside before the palace a great garden w...</td>\n",
       "      <td>-1.054013</td>\n",
       "      <td>0.450007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37c1b32fb</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Once upon a time there were Three Bears who li...</td>\n",
       "      <td>0.247197</td>\n",
       "      <td>0.510845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id url_legal license  \\\n",
       "0  c12129c31       NaN     NaN   \n",
       "1  85aa80a4c       NaN     NaN   \n",
       "2  b69ac6792       NaN     NaN   \n",
       "3  dd1000b26       NaN     NaN   \n",
       "4  37c1b32fb       NaN     NaN   \n",
       "\n",
       "                                             excerpt    target  standard_error  \n",
       "0  When the young people returned to the ballroom... -0.340259        0.464009  \n",
       "1  All through dinner time, Mrs. Fayre was somewh... -0.315372        0.480805  \n",
       "2  As Roger had predicted, the snow departed as q... -0.580118        0.476676  \n",
       "3  And outside before the palace a great garden w... -1.054013        0.450007  \n",
       "4  Once upon a time there were Three Bears who li...  0.247197        0.510845  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data =  pd.read_csv(\"commonlitreadabilityprize/train.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced 1327 words with characters with an ordinal >= 128 in the test data.\n"
     ]
    }
   ],
   "source": [
    "train_data.excerpt = train_data['excerpt'].apply(str)\n",
    "non_ascii_words = remove_ascii_words(train_data)\n",
    "\n",
    "print(\"Replaced {} words with characters with an ordinal >= 128 in the test data.\".format(\n",
    "    len(non_ascii_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained W2V on Google News\n",
    "W2Vmodel = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_preprocessing(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(train_data[train_data.tokenized_sentences.str.len() == 0].index, inplace= True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 25633.\n",
      "Number of texts: 2834.\n"
     ]
    }
   ],
   "source": [
    "#create dictionary with all sentences\n",
    "sentences = []\n",
    "for sentence_group in train_data.tokenized_sentences:\n",
    "    sentences.extend(sentence_group)\n",
    "\n",
    "print(\"Number of sentences: {}.\".format(len(sentences)))\n",
    "print(\"Number of texts: {}.\".format(len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_features(w2v_model, sentence_group):\n",
    "    \"\"\" Transform a sentence_group (containing multiple lists\n",
    "    of words) into a feature vector. It averages out all the\n",
    "    word vectors of the sentence_group.\n",
    "    \"\"\"\n",
    "    words = np.concatenate(sentence_group)  # words in text\n",
    "    index2word_set = set(w2v_model.index_to_key) # set(w2v_model.wv.vocab.keys())  # words known to model\n",
    "    \n",
    "    featureVec = np.zeros(w2v_model.vector_size, dtype=\"float32\")\n",
    "    \n",
    "    # Initialize a counter for number of words in a review\n",
    "    nwords = 0\n",
    "    # Loop over each word in the comment and, if it is in the model's vocabulary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            featureVec = np.add(featureVec, w2v_model[word])\n",
    "            nwords += 1.\n",
    "\n",
    "    # Divide the result by the number of words to get the average\n",
    "    if nwords > 0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['w2v_features'] = list(map(lambda sen_group:\n",
    "                                     get_w2v_features(W2Vmodel, sen_group),\n",
    "                                     train_data.tokenized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"w2v_resh_features\"] = train_data[\"w2v_features\"].apply(lambda x : x.reshape(1,-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save w2v features\n",
    "#train_data.to_csv(\"w2v_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_w2v = train_data.w2v_resh_features[0]\n",
    "for i in range(1, len(train_data)):\n",
    "    arr_w2v = np.vstack((arr_w2v, train_data.w2v_resh_features[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_emb = umap.UMAP(n_neighbors= 15, n_components = 2, target_metric = 'l1' , n_epochs = 500).fit_transform(arr_w2v, y=train_data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(*umap_emb.T, s=3, c=train_data.target, cmap='Spectral', alpha=1.0)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "ax.patch.set_facecolor('black')\n",
    "fg_color = 'black'\n",
    "cbar = plt.colorbar()\n",
    "plt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color=fg_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    arr_w2v, train_data.target.values, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = umap.UMAP(n_neighbors= 15, n_components = 2, target_metric = 'l1' , n_epochs = 1000).fit(X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_embedding = mapper.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(*mapper.embedding_.T, s=3, c=y_train, cmap='Spectral', alpha=1.0)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "ax.patch.set_facecolor('black')\n",
    "fg_color = 'black'\n",
    "cbar = plt.colorbar()\n",
    "plt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color=fg_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(*val_embedding.T, s=3, c=y_val, cmap='Spectral', alpha=1.0)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "ax.patch.set_facecolor('black')\n",
    "fg_color = 'black'\n",
    "cbar = plt.colorbar()\n",
    "plt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color=fg_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression on W2V\n",
    "Proviamo a vedere se le feature del W2V sono rilevanti per-se "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = GradientBoostingRegressor(random_state=0)\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.standard_error.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(y_val, y_pred) #0.44 not bad, let's see on test :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the cosine similarity\n",
    "#cosine_similarity(v_1.reshape(1,-1), v_2.reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_train_w2v(X_train, y_train, times = 5):\n",
    "    \n",
    "    augmented_w2v_X = X_train.copy()\n",
    "    augmented_w2v_y = y_train.copy()\n",
    "    \n",
    "    for j in range(0, times - 1):\n",
    "        for i in range(0, X_train.shape[0]):\n",
    "        \n",
    "            new_w2v = X_train[i,:] + np.random.uniform(1,8)*1e-3*np.random.randn(300)       # np.random.uniform(1,8)*1e-3*np.random.randn(300)\n",
    "            augmented_w2v_X = np.vstack((augmented_w2v_X, new_w2v))\n",
    "            \n",
    "            new_y = y_train[i] +  np.random.choice([-1,1])*0.10 #*0.5\n",
    "            augmented_w2v_y = np.append(augmented_w2v_y, new_y)\n",
    "        \n",
    "    return augmented_w2v_X, augmented_w2v_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_X_train, aug_Y_train = augment_train_w2v(X_train, y_train, times = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_X_train, aug_Y_train = unison_shuffled_copies(aug_X_train, aug_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = GradientBoostingRegressor(random_state=0)\n",
    "reg.fit(aug_X_train, aug_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(y_val, y_pred) #0.44 not bad, let's see on test :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper_2 = umap.UMAP(n_neighbors= 15, n_components = 2,min_dist = 0.1, target_metric = 'l1' , n_epochs = 1000).fit_transform(aug_X_train, y=aug_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 10))\n",
    "plt.scatter(*mapper_2.T, s=3, c=aug_Y_train, cmap='Spectral', alpha=1.0)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "ax.patch.set_facecolor('black')\n",
    "fg_color = 'black'\n",
    "cbar = plt.colorbar()\n",
    "plt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color=fg_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data =  pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.excerpt = test_data['excerpt'].apply(str)\n",
    "non_ascii_words = remove_ascii_words(test_data)\n",
    "\n",
    "print(\"Replaced {} words with characters with an ordinal >= 128 in the test data.\".format(\n",
    "    len(non_ascii_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_preprocessing(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.drop(test_data[test_data.tokenized_sentences.str.len() == 0].index, inplace= True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary with all sentences\n",
    "sentences_test = []\n",
    "for sentence_group in test_data.tokenized_sentences:\n",
    "    sentences_test.extend(sentence_group)\n",
    "\n",
    "print(\"Number of sentences: {}.\".format(len(sentences)))\n",
    "print(\"Number of texts: {}.\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['w2v_features'] = list(map(lambda sen_group:\n",
    "                                     get_w2v_features(W2Vmodel, sen_group),\n",
    "                                     test_data.tokenized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"w2v_resh_features\"] = test_data[\"w2v_features\"].apply(lambda x : x.reshape(1,-1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_w2v_test = test_data.w2v_resh_features[0]\n",
    "for i in range(1, len(test_data)):\n",
    "    arr_w2v_test = np.vstack((arr_w2v_test, test_data.w2v_resh_features[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
