{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "configured-appendix",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-06-02T22:37:40.355260Z",
     "iopub.status.busy": "2021-06-02T22:37:40.354493Z",
     "iopub.status.idle": "2021-06-02T22:37:52.060394Z",
     "shell.execute_reply": "2021-06-02T22:37:52.059778Z",
     "shell.execute_reply.started": "2021-06-02T22:34:03.262095Z"
    },
    "papermill": {
     "duration": 11.736989,
     "end_time": "2021-06-02T22:37:52.060550",
     "exception": false,
     "start_time": "2021-06-02T22:37:40.323561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVR\n",
    "from catboost import CatBoostRegressor, Pool, CatBoost\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.optimizer import Optimizer\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.optim.lr_scheduler import (CosineAnnealingWarmRestarts, CosineAnnealingLR, \n",
    "                                      ReduceLROnPlateau)\n",
    "\n",
    "from transformers import (AutoModel, AutoTokenizer, \n",
    "                          AutoModelForSequenceClassification,get_constant_schedule_with_warmup)\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nominated-layout",
   "metadata": {
    "papermill": {
     "duration": 0.012039,
     "end_time": "2021-06-02T22:37:52.085706",
     "exception": false,
     "start_time": "2021-06-02T22:37:52.073667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Augmented Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "legal-natural",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T22:37:52.117957Z",
     "iopub.status.busy": "2021-06-02T22:37:52.117394Z",
     "iopub.status.idle": "2021-06-02T22:37:52.344685Z",
     "shell.execute_reply": "2021-06-02T22:37:52.343751Z",
     "shell.execute_reply.started": "2021-06-02T22:34:03.622978Z"
    },
    "papermill": {
     "duration": 0.246701,
     "end_time": "2021-06-02T22:37:52.344830",
     "exception": false,
     "start_time": "2021-06-02T22:37:52.098129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../input/commonlitreadabilityprize/train.csv')\n",
    "aug_train = pd.read_csv(\"../input/scrapedcommonlit/external_df.csv\")\n",
    "aug_train = aug_train.rename(columns = {\"usable_external\": \"excerpt\"})\n",
    "aug_train[\"target\"] = pd.merge(aug_train, train_data , how='inner', on=['id']).target\n",
    "aug_train[\"standard_error\"] = pd.merge(aug_train, train_data , how='inner', on=['id']).standard_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "proved-birth",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T22:37:52.377270Z",
     "iopub.status.busy": "2021-06-02T22:37:52.376445Z",
     "iopub.status.idle": "2021-06-02T22:37:52.382208Z",
     "shell.execute_reply": "2021-06-02T22:37:52.381788Z",
     "shell.execute_reply.started": "2021-06-02T22:34:03.804690Z"
    },
    "papermill": {
     "duration": 0.025162,
     "end_time": "2021-06-02T22:37:52.382322",
     "exception": false,
     "start_time": "2021-06-02T22:37:52.357160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#for kfold  \n",
    "num_bins = 10 #int(np.floor(1 + np.log2(len(train_data))))\n",
    "train_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n",
    "aug_train.loc[:,'bins'] = pd.cut(aug_train['target'],bins=num_bins, labels=False)\n",
    "#bins = train_data.bins.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "attached-banks",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T22:37:52.419473Z",
     "iopub.status.busy": "2021-06-02T22:37:52.409040Z",
     "iopub.status.idle": "2021-06-02T22:37:53.283538Z",
     "shell.execute_reply": "2021-06-02T22:37:53.282692Z",
     "shell.execute_reply.started": "2021-06-02T22:34:03.991842Z"
    },
    "papermill": {
     "duration": 0.889232,
     "end_time": "2021-06-02T22:37:53.283667",
     "exception": false,
     "start_time": "2021-06-02T22:37:52.394435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 325/325 [00:00<00:00, 399.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# molto inefficiente, ma le righe sono poche e dovrebbe non metterci molto \n",
    "\n",
    "np.random.seed(137)\n",
    "frac_of_std = 10\n",
    "\n",
    "for row_idx in tqdm(range(0, aug_train.shape[0])):\n",
    "    #row_idx = 1\n",
    "    \n",
    "    # find substring \".\\n\", punto a capo per separare stringone scrapato \n",
    "    substr_list = []\n",
    "    fakeid_list = []\n",
    "    target_list = []\n",
    "    bins_list = []\n",
    "\n",
    "    id_ = aug_train.id[row_idx]\n",
    "    str_ = aug_train.excerpt[row_idx]\n",
    "    target_ = aug_train.target[row_idx]\n",
    "    std_ = aug_train.standard_error[row_idx]\n",
    "    bins_ = aug_train.bins[row_idx]\n",
    "    \n",
    "    idx_str = 0 \n",
    "    counter = 1 #generating fake ids for merging new dataframe\n",
    "\n",
    "    if row_idx == 0:\n",
    "        df = pd.DataFrame(list(zip(fakeid_list, substr_list, target_list, bins_list)),\n",
    "                   columns =['id', 'excerpt', 'target', 'bins'])\n",
    "\n",
    "    while idx_str > -1:\n",
    "        new_id = id_ + str(counter)    \n",
    "        idx_str = str_.find(\".\\n\", int(np.random.normal(971, 117)), len(str_))\n",
    "        substr_list.append(str_[:idx_str+2])\n",
    "        fakeid_list.append(new_id)\n",
    "        bins_list.append(bins_)\n",
    "        \n",
    "        new_target = np.random.normal(target_, std_/frac_of_std)\n",
    "        target_list.append(new_target)\n",
    "        \n",
    "        str_ = str_[idx_str+2:]\n",
    "        counter += 1\n",
    "\n",
    "    substr_list = substr_list[:-1] #remove last element\n",
    "    fakeid_list = fakeid_list[:-1] \n",
    "    target_list = target_list[:-1]\n",
    "    bins_list = bins_list[:-1]\n",
    "    #len(fakeid_list) == len(substr_list) == len(target_list) # safety check\n",
    "\n",
    "    df_ = pd.DataFrame(list(zip(fakeid_list, substr_list, target_list, bins_list)),   \n",
    "                   columns =['id', 'excerpt', 'target', 'bins']) # pivot dataframe\n",
    "    #print(df_.shape) #safety check\n",
    "\n",
    "    df = df.append(df_)\n",
    "    \n",
    "    \n",
    "new_aug_train = df \n",
    "new_train = train_data[~train_data.id.isin(aug_train.id)]\n",
    "new_train = new_train.drop(columns=['url_legal', 'license', 'standard_error'])\n",
    "new_train = new_train.append(new_aug_train)\n",
    "new_train = new_train.sample(frac=1)\n",
    "new_train = new_train.reset_index(drop = True)\n",
    "\n",
    "# facciamo in modo di riprodurre la stessa distribuzione di partenza per i target anche \n",
    "# nel dataset augmented \n",
    "\n",
    "frac_to_keep = 1 / min(new_train.bins.value_counts() / train_data.bins.value_counts())\n",
    "\n",
    "# sono solo 10 bins, non avevo voglia di fare le cose bene .... \n",
    "train_0 = new_train[new_train.bins == 0].sample(n = round((train_data.bins.value_counts() / frac_to_keep)[0]))\n",
    "train_1 = new_train[new_train.bins == 1].sample(n = round((train_data.bins.value_counts() / frac_to_keep)[1]))\n",
    "train_2 = new_train[new_train.bins == 2].sample(n = round((train_data.bins.value_counts() / frac_to_keep)[2]))\n",
    "train_3 = new_train[new_train.bins == 3].sample(n = round((train_data.bins.value_counts() / frac_to_keep)[3]))\n",
    "train_4 = new_train[new_train.bins == 4].sample(n = round((train_data.bins.value_counts() / frac_to_keep)[4]))\n",
    "train_5 = new_train[new_train.bins == 5].sample(n = round((train_data.bins.value_counts() / frac_to_keep)[5]))\n",
    "train_6 = new_train[new_train.bins == 6].sample(n = round((train_data.bins.value_counts() / frac_to_keep)[6]))\n",
    "train_7 = new_train[new_train.bins == 7].sample(n = round((train_data.bins.value_counts() / frac_to_keep)[7]))\n",
    "train_8 = new_train[new_train.bins == 8].sample(n = round((train_data.bins.value_counts() / frac_to_keep)[8]))\n",
    "train_9 = new_train[new_train.bins == 9].sample(n = round((train_data.bins.value_counts() / frac_to_keep)[9]))\n",
    "train_data =  train_0.append([train_1, train_2, train_3, train_4, train_5, train_6, \\\n",
    "                             train_7, train_8, train_9])\n",
    "\n",
    "train_data = train_data.sample(frac=1)\n",
    "train_data = train_data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "virgin-pursuit",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T22:37:53.317835Z",
     "iopub.status.busy": "2021-06-02T22:37:53.317090Z",
     "iopub.status.idle": "2021-06-02T22:37:53.320010Z",
     "shell.execute_reply": "2021-06-02T22:37:53.319551Z",
     "shell.execute_reply.started": "2021-06-02T22:34:06.556949Z"
    },
    "papermill": {
     "duration": 0.02157,
     "end_time": "2021-06-02T22:37:53.320123",
     "exception": false,
     "start_time": "2021-06-02T22:37:53.298553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# previous index reset\n",
    "train_data = train_data.set_index('id')\n",
    "aug_train = aug_train.set_index('id')\n",
    "train_data.update(aug_train)\n",
    "train_data.reset_index(inplace=True)\n",
    "#aug_train.reset_index(inplace = True)\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "comprehensive-mailing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T22:37:53.356078Z",
     "iopub.status.busy": "2021-06-02T22:37:53.355564Z",
     "iopub.status.idle": "2021-06-02T22:37:53.369152Z",
     "shell.execute_reply": "2021-06-02T22:37:53.368702Z",
     "shell.execute_reply.started": "2021-06-02T22:34:06.566867Z"
    },
    "papermill": {
     "duration": 0.034685,
     "end_time": "2021-06-02T22:37:53.369269",
     "exception": false,
     "start_time": "2021-06-02T22:37:53.334584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('../input/commonlitreadabilityprize/test.csv')\n",
    "sample = pd.read_csv('../input/commonlitreadabilityprize/sample_submission.csv')\n",
    "\n",
    "target = train_data['target'].to_numpy()\n",
    "\n",
    "#for kfold  \n",
    "#num_bins = 10 #int(np.floor(1 + np.log2(len(train_data))))\n",
    "#train_data.loc[:,'bins'] = pd.cut(train_data['target'],bins=num_bins,labels=False)\n",
    "bins = train_data.bins.to_numpy(dtype = np.int16)\n",
    "\n",
    "def rmse_score(y_true,y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faced-indian",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T22:37:53.404309Z",
     "iopub.status.busy": "2021-06-02T22:37:53.403648Z",
     "iopub.status.idle": "2021-06-02T22:37:53.410208Z",
     "shell.execute_reply": "2021-06-02T22:37:53.409257Z",
     "shell.execute_reply.started": "2021-06-02T22:34:06.588702Z"
    },
    "papermill": {
     "duration": 0.026361,
     "end_time": "2021-06-02T22:37:53.410324",
     "exception": false,
     "start_time": "2021-06-02T22:37:53.383963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'batch_size':128,\n",
    "    'max_len':256,\n",
    "    'seed':42,\n",
    "}\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONASSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(seed=config['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cordless-ticket",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T22:37:53.445883Z",
     "iopub.status.busy": "2021-06-02T22:37:53.445120Z",
     "iopub.status.idle": "2021-06-02T22:37:53.447661Z",
     "shell.execute_reply": "2021-06-02T22:37:53.447273Z",
     "shell.execute_reply.started": "2021-06-02T22:34:06.607116Z"
    },
    "papermill": {
     "duration": 0.022604,
     "end_time": "2021-06-02T22:37:53.447768",
     "exception": false,
     "start_time": "2021-06-02T22:37:53.425164",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CLRPDataset(nn.Module):\n",
    "    def __init__(self,df,tokenizer,max_len=128):\n",
    "        self.excerpt = df['excerpt'].to_numpy()\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        encode = self.tokenizer(self.excerpt[idx],\n",
    "                                return_tensors='pt',\n",
    "                                max_length=self.max_len,\n",
    "                                padding='max_length',\n",
    "                                truncation=True)  \n",
    "        return encode\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "foster-prototype",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T22:37:53.484977Z",
     "iopub.status.busy": "2021-06-02T22:37:53.484176Z",
     "iopub.status.idle": "2021-06-02T22:37:53.487087Z",
     "shell.execute_reply": "2021-06-02T22:37:53.486626Z",
     "shell.execute_reply.started": "2021-06-02T22:34:06.620772Z"
    },
    "papermill": {
     "duration": 0.024674,
     "end_time": "2021-06-02T22:37:53.487194",
     "exception": false,
     "start_time": "2021-06-02T22:37:53.462520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_embeddings(df,path,plot_losses=True, verbose=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"{device} is used\")\n",
    "            \n",
    "    MODEL_PATH = path\n",
    "    model = AutoModel.from_pretrained(MODEL_PATH)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    ds = CLRPDataset(df,tokenizer,config['max_len'])\n",
    "    dl = DataLoader(ds,\n",
    "                  batch_size = config[\"batch_size\"],\n",
    "                  shuffle=False,\n",
    "                  num_workers = 4,\n",
    "                  pin_memory=True,\n",
    "                  drop_last=False\n",
    "                 )\n",
    "        \n",
    "    embeddings = list()\n",
    "    with torch.no_grad():\n",
    "        for i, inputs in tqdm(enumerate(dl)):\n",
    "            inputs = {key:val.reshape(val.shape[0],-1).to(device) for key,val in inputs.items()}\n",
    "            outputs = model(**inputs)\n",
    "            outputs = outputs[0][:,0].detach().cpu().numpy()\n",
    "            embeddings.extend(outputs)\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "opened-timing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T22:37:53.571158Z",
     "iopub.status.busy": "2021-06-02T22:37:53.570481Z",
     "iopub.status.idle": "2021-06-02T22:41:52.692020Z",
     "shell.execute_reply": "2021-06-02T22:41:52.691562Z",
     "shell.execute_reply.started": "2021-06-02T22:34:06.635650Z"
    },
    "papermill": {
     "duration": 239.190398,
     "end_time": "2021-06-02T22:41:52.692156",
     "exception": false,
     "start_time": "2021-06-02T22:37:53.501758",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/modelf1 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "35it [00:35,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/modelf1 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "1it [00:00,  5.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/modelf2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "35it [00:33,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/modelf2 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "1it [00:00,  4.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/modelf3 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "35it [00:34,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/modelf3 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "1it [00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/modelf4 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "35it [00:33,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/modelf4 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "1it [00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/modelf5 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "35it [00:34,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is used\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at ../input/modelf5 and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "1it [00:00,  4.86it/s]\n"
     ]
    }
   ],
   "source": [
    "train_embeddings1 =  get_embeddings(train_data,'../input/modelf1')\n",
    "test_embeddings1 = get_embeddings(test_data,'../input/modelf1')\n",
    "\n",
    "\n",
    "train_embeddings2 =  get_embeddings(train_data,'../input/modelf2')\n",
    "test_embeddings2 = get_embeddings(test_data,'../input/modelf2')\n",
    "\n",
    "train_embeddings3 =  get_embeddings(train_data,'../input/modelf3')\n",
    "test_embeddings3 = get_embeddings(test_data,'../input/modelf3')\n",
    "\n",
    "train_embeddings4 =  get_embeddings(train_data,'../input/modelf4')\n",
    "test_embeddings4 = get_embeddings(test_data,'../input/modelf4')\n",
    "\n",
    "train_embeddings5 =  get_embeddings(train_data,'../input/modelf5')\n",
    "test_embeddings5 = get_embeddings(test_data,'../input/modelf5')\n",
    "\"\"\"\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-watershed",
   "metadata": {
    "papermill": {
     "duration": 0.071184,
     "end_time": "2021-06-02T22:41:52.833424",
     "exception": false,
     "start_time": "2021-06-02T22:41:52.762240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Neural Tangent Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "answering-authority",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T22:41:52.980219Z",
     "iopub.status.busy": "2021-06-02T22:41:52.979555Z",
     "iopub.status.idle": "2021-06-02T22:42:45.591409Z",
     "shell.execute_reply": "2021-06-02T22:42:45.590894Z",
     "shell.execute_reply.started": "2021-06-02T22:34:49.255847Z"
    },
    "papermill": {
     "duration": 52.68756,
     "end_time": "2021-06-02T22:42:45.591539",
     "exception": false,
     "start_time": "2021-06-02T22:41:52.903979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/roberta/frozendict-2.0.2-py3-none-any.whl\r\n",
      "Installing collected packages: frozendict\r\n",
      "Successfully installed frozendict-2.0.2\r\n",
      "Processing /kaggle/input/roberta/neural_tangents-0.3.6-py2.py3-none-any.whl\r\n",
      "Requirement already satisfied: frozendict>=1.2 in /opt/conda/lib/python3.7/site-packages (from neural-tangents==0.3.6) (2.0.2)\r\n",
      "Requirement already satisfied: jax>=0.1.77 in /opt/conda/lib/python3.7/site-packages (from neural-tangents==0.3.6) (0.2.12)\r\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from jax>=0.1.77->neural-tangents==0.3.6) (0.12.0)\r\n",
      "Requirement already satisfied: numpy>=1.12 in /opt/conda/lib/python3.7/site-packages (from jax>=0.1.77->neural-tangents==0.3.6) (1.19.5)\r\n",
      "Requirement already satisfied: opt-einsum in /opt/conda/lib/python3.7/site-packages (from jax>=0.1.77->neural-tangents==0.3.6) (3.3.0)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py->jax>=0.1.77->neural-tangents==0.3.6) (1.15.0)\r\n",
      "Installing collected packages: neural-tangents\r\n",
      "Successfully installed neural-tangents-0.3.6\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/roberta/frozendict-2.0.2-py3-none-any.whl\n",
    "!pip install ../input/roberta/neural_tangents-0.3.6-py2.py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "commercial-edwards",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T22:42:45.752339Z",
     "iopub.status.busy": "2021-06-02T22:42:45.749947Z",
     "iopub.status.idle": "2021-06-02T22:42:46.351816Z",
     "shell.execute_reply": "2021-06-02T22:42:46.351039Z",
     "shell.execute_reply.started": "2021-06-02T22:35:44.261292Z"
    },
    "papermill": {
     "duration": 0.68614,
     "end_time": "2021-06-02T22:42:46.351956",
     "exception": false,
     "start_time": "2021-06-02T22:42:45.665816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import random\n",
    "from neural_tangents import stax\n",
    "import neural_tangents as nt\n",
    "\n",
    "def get_preds_ntk(X,y,X_test,bins=bins,nfolds=5):\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=nfolds)\n",
    "    scores = list()\n",
    "    preds = np.zeros((X_test.shape[0]))\n",
    "    \n",
    "    for k, (train_idx,valid_idx) in enumerate(kfold.split(X,bins)):\n",
    "        X_train,y_train = X[train_idx], y[train_idx]\n",
    "        X_valid,y_valid = X[valid_idx], y[valid_idx]\n",
    "\n",
    "        ResBlock = stax.serial(\n",
    "                        stax.FanOut(2),\n",
    "                        stax.parallel(\n",
    "                            stax.serial(\n",
    "                                stax.Erf(),\n",
    "                                stax.Dense(1, W_std=1.25, b_std=0.0),\n",
    "                                stax.Erf(),\n",
    "                                stax.Dense(1, W_std=1.25, b_std=0.0),\n",
    "                                stax.Erf(),\n",
    "                                stax.Dense(1, W_std=1.25, b_std=0.0),\n",
    "                            ),\n",
    "                            stax.Identity(),\n",
    "                        ),\n",
    "                        stax.FanInSum()\n",
    "                    )\n",
    "\n",
    "        init_fn, apply_fn, kernel_fn = stax.serial(\n",
    "                stax.Dense(1, W_std=1.0, b_std=0),\n",
    "                ResBlock, ResBlock, stax.Erf(),\n",
    "                stax.Dense(1, W_std=2.5, b_std=0.1)\n",
    "        )\n",
    "\n",
    "        key = random.PRNGKey(10)\n",
    "        _, params = init_fn(key, input_shape=X_train.shape)\n",
    "        predict_fn = nt.predict.gradient_descent_mse_ensemble(kernel_fn,\n",
    "                                                                  X_train,\n",
    "                                                                  y_train[:,np.newaxis],\n",
    "                                                                  diag_reg=1e-1,\n",
    "                                                                  lr=1)\n",
    "        \n",
    "        prediction = predict_fn(x_test=X_valid, get='nngp', t=None)#model.predict(X_valid)\n",
    "        score = rmse_score(prediction,y_valid)\n",
    "        print(f'Fold {k} , rmse score: {score}')\n",
    "        scores.append(score)\n",
    "        preds += predict_fn(x_test=X_test, get='nngp', t=None)#model.predict(X_test)\n",
    "        \n",
    "    print(\"mean rmse\",np.mean(scores))\n",
    "    return np.array(preds)/nfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "spoken-adoption",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T22:42:46.501552Z",
     "iopub.status.busy": "2021-06-02T22:42:46.500903Z",
     "iopub.status.idle": "2021-06-02T22:46:06.850499Z",
     "shell.execute_reply": "2021-06-02T22:46:06.851061Z",
     "shell.execute_reply.started": "2021-06-02T22:35:44.281816Z"
    },
    "papermill": {
     "duration": 200.427336,
     "end_time": "2021-06-02T22:46:06.851269",
     "exception": false,
     "start_time": "2021-06-02T22:42:46.423933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 , rmse score: 0.5663396585472567\n",
      "Fold 1 , rmse score: 0.5698152964930118\n",
      "Fold 2 , rmse score: 0.6002650004424258\n",
      "Fold 3 , rmse score: 0.5752597303446432\n",
      "Fold 4 , rmse score: 0.5582475774325402\n",
      "mean rmse 0.5739854526519756\n",
      "Fold 0 , rmse score: 0.5755103583961869\n",
      "Fold 1 , rmse score: 0.5794721355235051\n",
      "Fold 2 , rmse score: 0.608997244927859\n",
      "Fold 3 , rmse score: 0.5833743779899584\n",
      "Fold 4 , rmse score: 0.5614220771076076\n",
      "mean rmse 0.5817552387890235\n",
      "Fold 0 , rmse score: 0.5318793235676887\n",
      "Fold 1 , rmse score: 0.5312760357265813\n",
      "Fold 2 , rmse score: 0.5609441206619032\n",
      "Fold 3 , rmse score: 0.5327352236319931\n",
      "Fold 4 , rmse score: 0.5212616574563316\n",
      "mean rmse 0.5356192722088996\n",
      "Fold 0 , rmse score: 0.5493700003215523\n",
      "Fold 1 , rmse score: 0.5471054897289109\n",
      "Fold 2 , rmse score: 0.5814445577407403\n",
      "Fold 3 , rmse score: 0.5597801307737409\n",
      "Fold 4 , rmse score: 0.5547992207297855\n",
      "mean rmse 0.558499879858946\n",
      "Fold 0 , rmse score: 0.5581239344485001\n",
      "Fold 1 , rmse score: 0.5410996055709191\n",
      "Fold 2 , rmse score: 0.5860194918625122\n",
      "Fold 3 , rmse score: 0.552854120533719\n",
      "Fold 4 , rmse score: 0.5500312104033369\n",
      "mean rmse 0.5576256725637976\n"
     ]
    }
   ],
   "source": [
    "ntk_preds1 = get_preds_ntk(train_embeddings1,target,test_embeddings1)\n",
    "\n",
    "ntk_preds2 = get_preds_ntk(train_embeddings2,target,test_embeddings2)\n",
    "ntk_preds3 = get_preds_ntk(train_embeddings3,target,test_embeddings3)\n",
    "ntk_preds4 = get_preds_ntk(train_embeddings4,target,test_embeddings4)\n",
    "ntk_preds5 = get_preds_ntk(train_embeddings5,target,test_embeddings5)\n",
    "\"\"\"\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "southern-viewer",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T22:46:07.017360Z",
     "iopub.status.busy": "2021-06-02T22:46:07.015814Z",
     "iopub.status.idle": "2021-06-02T22:46:07.018180Z",
     "shell.execute_reply": "2021-06-02T22:46:07.018587Z"
    },
    "papermill": {
     "duration": 0.086457,
     "end_time": "2021-06-02T22:46:07.018725",
     "exception": false,
     "start_time": "2021-06-02T22:46:06.932268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ntk_preds = (ntk_preds1 + ntk_preds2 + ntk_preds3 + ntk_preds4 + ntk_preds5)/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "different-render",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-02T22:46:07.182623Z",
     "iopub.status.busy": "2021-06-02T22:46:07.181762Z",
     "iopub.status.idle": "2021-06-02T22:46:07.195556Z",
     "shell.execute_reply": "2021-06-02T22:46:07.195107Z"
    },
    "papermill": {
     "duration": 0.097639,
     "end_time": "2021-06-02T22:46:07.195673",
     "exception": false,
     "start_time": "2021-06-02T22:46:07.098034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample.target = ntk_preds\n",
    "sample.to_csv('submission.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 516.845766,
   "end_time": "2021-06-02T22:46:10.306511",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-02T22:37:33.460745",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
